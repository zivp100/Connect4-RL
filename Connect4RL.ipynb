{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to play Connect4 using Reinforcement Learning\n",
    "Improveing on the orignial algorithm presented at : https://www.kaggle.com/alexisbcook/deep-reinforcement-learning\n",
    "\n",
    "This will have 3 sections:\n",
    "1. Building and training the original agent descibed on Kaggle\n",
    "2. Building and training an improved agent who plays against itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Building and training the original agent\n",
    "1. Init\n",
    "2. Build Gym environment \n",
    "3. Build the NN\n",
    "4. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "#from learntools.core import binder\n",
    "#binder.bind(globals())\n",
    "#from learntools.game_ai.ex4 import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!pip install 'tensorflow==1.15.0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
    "!pip install \"stable-baselines[mpi]==2.9.0\"\n",
    "\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO1, PPO2, A2C, ACER, ACKTR, TRPO\n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"log/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#initalizing variables        \n",
    "iterations = 10\n",
    "steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Connect4 Gym Enviroment\n",
    "# change the reward function from original implementation (see note below)\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: \n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for predicting action values\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "         \n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn by playing against random agent\n",
    "\n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym(agent2=\"random\")\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True) # Logging progress\n",
    "vec_env = DummyVecEnv([lambda: monitor_env]) # Create a vectorized environment\n",
    " \n",
    "# Initialize agent\n",
    "simple_model = PPO1(CustomCnnPolicy, vec_env, verbose=0)\n",
    "print(\"Training started...\")\n",
    "\n",
    "iterations = 25\n",
    "for x in range(iterations):\n",
    "  # Train agent\n",
    "  simple_model.learn(total_timesteps=steps)\n",
    "\n",
    "  # Print results\n",
    "  with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "      firstline = fh.readline()\n",
    "      assert firstline[0] == '#'\n",
    "      df1 = pd.read_csv(fh, index_col=None)['r']\n",
    "      df1 = df1.tail(steps)\n",
    "      print(\"Iteration=\", x, \" Score=\",df1.mean())\n",
    "    \n",
    "print(\"Training Finished!\",\" Score=\",df1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kaggle_environments import make, evaluate\n",
    "\n",
    "def learnt_from_random_agent(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = simple_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "    \n",
    "# Utility function used to compare agents performance\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config ={'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    return np.round(outcomes.count([1,-1])/len(outcomes), 2), np.round(outcomes.count([-1,1])/len(outcomes), 2)\n",
    "    \n",
    "  #  print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "  #  print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "  #  print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]))\n",
    "  #  print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training using random player finished!\")\n",
    "get_win_percentages(agent1=\"random\", agent2=learn_from_self_play_agent)\n",
    "print(\"Random agent win =\",win1, \"My agent win = \",win2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Building and training the self-play agent\n",
    "1. Build the NN (we use the same NN as before)\n",
    "3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the NN\n",
    "\n",
    "def extra_layer_cnn(scaled_images, **kwargs):\n",
    "    # Original implementation\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "        \n",
    "class CustomCnnPolicy2(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy2, self).__init__(*args, **kwargs, cnn_extractor=extra_layer_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self play / learn\n",
    "from random import choice\n",
    "        \n",
    "# Create the initial ConnectFour environment\n",
    "env = ConnectFourGym(agent2=\"random\")\n",
    "vec_env = DummyVecEnv([lambda: env]) # Create a vectorized environment\n",
    "\n",
    "# Initialize agents\n",
    "selfplay1_model = PPO1(CustomCnnPolicy2, vec_env, verbose=0) \n",
    "selfplay2_model = PPO1(CustomCnnPolicy2, vec_env, verbose=0) \n",
    "\n",
    "def agent1_play(obs, config):\n",
    "        # Use the best model to select a column\n",
    "        col, _ = selfplay1_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "        # Check if selected column is valid\n",
    "        is_valid = (obs['board'][int(col)] == 0)\n",
    "        # If not valid, select random move. \n",
    "        if is_valid:\n",
    "            return int(col)\n",
    "        else:\n",
    "            return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "        \n",
    "def agent2_play(obs, config):\n",
    "        #return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n",
    "        # Use the best model to select a column\n",
    "        col, _ = selfplay2_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "        # Check if selected column is valid\n",
    "        #is_valid = (obs['board'][int(col)] == 0)\n",
    "        # If not valid, select random move. \n",
    "        #if is_valid:\n",
    "        return int(col)\n",
    "        #else:\n",
    "        #    return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])        \n",
    "        \n",
    "\n",
    "win2=0\n",
    "steps=10000\n",
    "counter=1\n",
    "\n",
    "win1 , win2 = get_win_percentages(agent1=agent1_play, agent2=\"random\") \n",
    "print(\"Before training stated. Agent win =\",win1, \"Random win = \",win2) \n",
    "    \n",
    "print(\"Training started...\")    \n",
    "while (win1 < 0.85): #playing aginst again that was trainer from random\n",
    "    # selfplay_model.save(\"my_model\")\n",
    "    # copy_model = PPO1.load(\"my_model\")\n",
    "    \n",
    "    # Build the new env base on the existing algorithm\n",
    "    selfplay1_model.save(\"my_model\") \n",
    "    selfplay2_model = PPO1.load(\"my_model\")\n",
    "    env = ConnectFourGym(agent2=agent2_play)\n",
    "    vec_env = DummyVecEnv([lambda: env]) # Create a vectorized environment\n",
    "    \n",
    "    # Keep training the model\n",
    "    selfplay1_model.learn(total_timesteps=10000)\n",
    "      \n",
    "    # Measure sucess    \n",
    "    win1 , win2 = get_win_percentages(agent1=agent1_play, agent2=\"random\") \n",
    "    print(\"iteration=\", counter, \" Agent win =\",win1, \"Random win = \",win2) \n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results\n",
    "get_win_percentages(agent1=\"random\", agent2=learn_from_self_play_agent)\n",
    "print(\"Random agent win =\",win1, \"My agent win = \",win2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kaggle_environments import make, evaluate\n",
    "\n",
    "# Utility function used to compare agents performance\n",
    "def get_win_percentages(agent1, agent2, n_rounds=1000):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "       \n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]))\n",
    "    print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0.5, 0.5]))\n",
    "\n",
    "    with open(os.path.join(log_dir, \"results.txt\"), 'w') as f:\n",
    "        print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2), file=f)\n",
    "        print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2), file=f)\n",
    "        print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]), file=f)\n",
    "        print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]), file=f)\n",
    "        print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0.5, 0.5]), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Saving the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import inspect\n",
    "#import os\n",
    "\n",
    "#def write_agent_to_file(function, file):\n",
    "#    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n",
    "#        f.write(inspect.getsource(function))\n",
    "#        print(function, \"written to\", file)\n",
    "\n",
    "#write_agent_to_file(learn_from_self_play_agent, \"submission.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, follow these steps:\n",
    "\n",
    "Begin by clicking on the blue Save Version button in the top right corner of this window. This will generate a pop-up window.\n",
    "Ensure that the Save and Run All option is selected, and then click on the blue Save button.\n",
    "This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
    "Click on the Output tab on the right of the screen. Then, click on the Submit to Competition button to submit your results to the leaderboard.\n",
    "Go to \"My Submissions\" to view your score and episodes being played.\n",
    "You have now successfully submitted to the competition!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
